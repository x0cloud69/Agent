import os
import requests
import json
from typing import List, Dict, Any, Optional, Mapping
from exa_search_service import exa_search_service

# Langchain imports
from langchain_community.llms import LlamaCpp, Ollama
from langchain_core.language_models.llms import BaseLLM
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain_core.callbacks.manager import CallbackManagerForLLMRun

# --- MCP Implementation ---

def check_ollama_server_sync(base_url: str = "http://127.0.0.1:11434") -> bool:
    """Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ ë™ê¸°ì ìœ¼ë¡œ í™•ì¸"""
    try:
        print(f"ğŸ” Ollama ì„œë²„ ìƒíƒœ í™•ì¸ ì¤‘... ({base_url})")
        # /api/tagsëŠ” ëª¨ë¸ ëª©ë¡ì„ ë°˜í™˜í•˜ë©°, ì„œë²„ ìƒíƒœ í™•ì¸ì— ì í•©í•©ë‹ˆë‹¤.
        response = requests.get(f"{base_url}/api/tags", timeout=3)
        if response.status_code == 200:
            print(f"   âœ… Ollama ì„œë²„ ì •ìƒ ì‘ë™")
            return True
        else:
            print(f"   âŒ Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
            return False
    except requests.exceptions.Timeout:
        print(f"   â° Ollama ì„œë²„ ì—°ê²° íƒ€ì„ì•„ì›ƒ")
        return False
    except requests.exceptions.RequestException as e:
        print(f"   âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

class MCP_LLM(BaseLLM):
    """MCP í”„ë¡œí† ì½œì„ í†µí•œ Ollama LLM ë˜í¼"""
    
    model_name: str = "qwen2.5:7b"
    base_url: str = "http://127.0.0.1:11434"
    temperature: float = 0.7
    top_p: float = 0.9
    
    def __init__(self, model_name: str = "qwen2.5:7b", base_url: str = "http://127.0.0.1:11434", temperature: float = 0.3, top_p: float = 0.9, **kwargs):
        super().__init__(model_name=model_name, base_url=base_url, temperature=temperature, top_p=top_p, **kwargs)
    
    @property
    def _llm_type(self) -> str:
        """LLM íƒ€ì…ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return "mcp_ollama"
 
    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """LLMì„ ì‹ë³„í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "model_name": self.model_name,
            "base_url": self.base_url,
            "temperature": self.temperature,
            "top_p": self.top_p,
        }

    def _generate(
        self,
        prompts: List[str],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ):
        """LangChainì˜ ìµœì‹  APIì— ë§ëŠ” _generate ë©”ì„œë“œ êµ¬í˜„"""
        from langchain_core.outputs import LLMResult, Generation
        
        generations = []
        for prompt in prompts:
            api_url = f"{self.base_url}/api/generate"
            
            # Ollama APIì— ë§ëŠ” payload êµ¬ì„±
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False, # ìŠ¤íŠ¸ë¦¬ë°ì€ ë¹„í™œì„±í™”
                "options": {
                    "temperature": self.temperature,
                    "top_p": self.top_p,
                },
            }
            
            # stop-sequenceê°€ ìˆìœ¼ë©´ payloadì— ì¶”ê°€
            if stop:
                payload["options"]["stop"] = stop

            try:
                print(f"   ğŸ“¤ Ollama API ìš”ì²­ ì „ì†¡ ì¤‘... (íƒ€ì„ì•„ì›ƒ: 120ì´ˆ)")
                response = requests.post(api_url, json=payload, timeout=120)
                response.raise_for_status()  # 200 OKê°€ ì•„ë‹ˆë©´ ì˜ˆì™¸ ë°œìƒ
                
                result = response.json()
                print(f"   âœ… Ollama ì›ë³¸ ì‘ë‹µ: {result}")  # ì „ì²´ ì‘ë‹µì„ ë¡œê·¸ë¡œ ì¶œë ¥
                response_text = result.get("response", "")
                print(f"   âœ… Ollama ì‘ë‹µ ì™„ë£Œ ({len(response_text)}ì)")
                
                # Generation ê°ì²´ ìƒì„±
                generation = Generation(text=response_text)
                generations.append([generation])
                
            except requests.exceptions.Timeout:
                raise ConnectionError(f"Ollama ì„œë²„ ì‘ë‹µ íƒ€ì„ì•„ì›ƒ (120ì´ˆ ì´ˆê³¼)")
            except requests.exceptions.RequestException as e:
                raise ConnectionError(f"Ollama ì„œë²„({api_url}) ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            except Exception as e:
                raise RuntimeError(f"Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return LLMResult(generations=generations)

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """LLMì„ í˜¸ì¶œí•˜ëŠ” ë©”ì¸ ë¡œì§ì…ë‹ˆë‹¤."""
        api_url = f"{self.base_url}/api/generate"
        
        # Qwen 3BëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—†ì´ ì§ˆë¬¸ë§Œ ì „ë‹¬
        enhanced_prompt = prompt
        print(f"[DEBUG] _callì—ì„œ Ollamaë¡œ ë³´ë‚¼ í”„ë¡¬í”„íŠ¸: {enhanced_prompt}")
        
        # Ollama APIì— ë§ëŠ” payload êµ¬ì„±
        payload = {
            "model": self.model_name,
            "prompt": enhanced_prompt,
            "stream": False, # ìŠ¤íŠ¸ë¦¬ë°ì€ ë¹„í™œì„±í™”
            "options": {
                "temperature": self.temperature,
                "top_p": self.top_p,
            },
        }
        print(f"[DEBUG] Ollamaì— ì „ë‹¬ë˜ëŠ” payload: {payload}")
        
        # stop-sequenceê°€ ìˆìœ¼ë©´ payloadì— ì¶”ê°€
        if stop:
            payload["options"]["stop"] = stop

        try:
            print(f"   ğŸ“¤ Ollama API ìš”ì²­ ì „ì†¡ ì¤‘... (íƒ€ì„ì•„ì›ƒ: 120ì´ˆ)")
            response = requests.post(api_url, json=payload, timeout=120)
            response.raise_for_status()  # 200 OKê°€ ì•„ë‹ˆë©´ ì˜ˆì™¸ ë°œìƒ
            
            result = response.json()
            print(f"   âœ… Ollama ì›ë³¸ ì‘ë‹µ: {result}")  # ì „ì²´ ì‘ë‹µì„ ë¡œê·¸ë¡œ ì¶œë ¥
            response_text = result.get("response", "")
            print(f"   âœ… Ollama ì‘ë‹µ ì™„ë£Œ ({len(response_text)}ì)")
            return response_text
            
        except requests.exceptions.Timeout:
            raise ConnectionError(f"Ollama ì„œë²„ ì‘ë‹µ íƒ€ì„ì•„ì›ƒ (120ì´ˆ ì´ˆê³¼)")
        except requests.exceptions.RequestException as e:
            raise ConnectionError(f"Ollama ì„œë²„({api_url}) ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        except Exception as e:
            raise RuntimeError(f"Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- Main Service Class ---

class LangChainAIService:
    def __init__(self, model_path: str = None, use_mcp: bool = True, use_ollama: bool = False, model_type: str = "qwen3"):
        """
        LangChainì„ ì‚¬ìš©í•œ AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (MCP ì¤‘ì‹¬)
        model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.gguf íŒŒì¼)
        use_mcp: MCP í”„ë¡œí† ì½œ ì‚¬ìš© ì—¬ë¶€ (ê¶Œì¥)
        use_ollama: Ollama ì§ì ‘ ì—°ê²° ì‚¬ìš© ì—¬ë¶€
        model_type: ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… ("qwen3", "qwen2", "llama2", "mistral" ë“±)
        """
        self.model = None
        self.model_path = model_path or os.getenv("MODEL_PATH", "models/qwen2.5-7b-instruct.gguf")
        self.use_mcp = use_mcp
        self.use_ollama = use_ollama
        self.model_type = model_type
        
        # MCP ìš°ì„  ì‚¬ìš© (ì•ˆì „í•˜ê³  í‘œì¤€í™”ëœ í†µì‹ )
        if self.use_mcp:
            self.load_mcp_model()
        elif self.use_ollama:
            self.load_ollama_model()
        else:
            # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
            if os.path.exists(self.model_path):
                self.load_model()
            else:
                print(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
                print("MCPë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤ (ì•ˆì „í•˜ê³  í‘œì¤€í™”ëœ í†µì‹ )")
                print("1. https://ollama.ai/ ì—ì„œ Ollama ë‹¤ìš´ë¡œë“œ")
                print("2. ollama pull qwen2.5:7b ì‹¤í–‰")
                print("3. ollama serve ì‹¤í–‰")
                print("4. use_mcp=Trueë¡œ ì„¤ì •")

    def load_mcp_model(self):
        """MCPë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ ë¡œë“œ (Ollama API ì§ì ‘ í˜¸ì¶œ ë°©ì‹)"""
        print("MCP ëª¨ë¸ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤ (Ollama API ì§ì ‘ í˜¸ì¶œ)...")
        
        # 1. Ollama ì„œë²„ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
        if not check_ollama_server_sync():
            print("âŒ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹™ë‹ˆë‹¤.")
            print("MCPë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ 'ollama serve' ëª…ë ¹ì–´ë¡œ ì„œë²„ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")
            self.model = None
            return

        print("âœ… Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
        
        # 2. ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ëª¨ë¸ëª… ì„¤ì • (Qwen 3B ìš°ì„ )
        if self.model_type == "qwen3":
            model_name = "qwen2.5:3b"  # Qwen 3B ëª¨ë¸
        elif self.model_type == "qwen2":
            model_name = "qwen2.5:7b"
        elif self.model_type == "llama2":
            model_name = "llama2:7b"
        elif self.model_type == "mistral":
            model_name = "mistral:7b"
        else:
            model_name = "qwen2.5:3b"  # ê¸°ë³¸ê°’ì„ Qwen 3Bë¡œ ë³€ê²½
            
        print(f"'{model_name}' ëª¨ë¸ì„ MCPë¥¼ í†µí•´ ë¡œë“œí•©ë‹ˆë‹¤...")
        
        try:
            # 3. MCP_LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            self.model = MCP_LLM(model_name=model_name)
            print(f"âœ… MCP ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
        except Exception as e:
            print(f"âŒ MCP ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.model = None

    def load_ollama_model(self):
        """Ollama ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ëª¨ë¸ ë¡œë“œ"""
        try:
            if self.model_type == "qwen3":
                model_name = "qwen2.5:7b"
                print("Qwen3 ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            elif self.model_type == "qwen2":
                model_name = "qwen2.5:7b"
                print("Qwen2.5 ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            elif self.model_type == "mistral":
                model_name = "mistral:7b"
                print("Mistral ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            else:
                model_name = "llama2:7b"
                print("Llama2 ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            
            self.model = Ollama(
                model=model_name,
                temperature=0.3,
                top_p=0.9
            )
            print(f"Ollama ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
            
        except Exception as e:
            print(f"Ollama ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹Œ ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
            print("1. https://ollama.ai/ ì—ì„œ Ollama ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜")
            print(f"2. 'ollama pull {self.model_type}' ì‹¤í–‰")
            print("3. 'ollama serve' ì‹¤í–‰")
            self.model = None

    def load_model(self):
        """LangChainì„ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ GGUF ëª¨ë¸ ë¡œë“œ"""
        try:
            callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])
            self.model = LlamaCpp(
                model_path=self.model_path,
                temperature=0.3,
                max_tokens=256,
                top_p=0.9,
                callback_manager=callback_manager,
                verbose=False,
                n_ctx=2048,
                n_threads=4,
                n_gpu_layers=0
            )
            print(f"ë¡œì»¬ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_path}")
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.model = None

    def generate_response(self, messages: List[Dict[str, str]], max_tokens: int = 512, use_web_search: bool = False) -> str:
        """ë©”ì‹œì§€ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„± (ì›¹ ê²€ìƒ‰ í¬í•¨)"""
        print(f"\n[DEBUG] generate_responseì— ì „ë‹¬ëœ messages: {messages}")
        if not self.is_model_loaded():
            return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        try:
            import time
            start_time = time.time()
            
            # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©
            last_message = messages[-1]['content'] if messages else ""
            print(f"[DEBUG] generate_responseì—ì„œ Ollamaë¡œ ë³´ë‚¼ í”„ë¡¬í”„íŠ¸: {last_message}")
            
            # Exa Search ë¹„í™œì„±í™” (ë¦¬ì†ŒìŠ¤ ì ˆì•½)
            # Qwen 3BëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—†ì´ ì§ˆë¬¸ë§Œ ì „ë‹¬
            enhanced_prompt = last_message
            print(f"   - Exa Search ë¹„í™œì„±í™”ë¨ (ë¦¬ì†ŒìŠ¤ ì ˆì•½)")
            print(f"   - Qwen 3B í”„ë¡¬í”„íŠ¸ ë‹¨ìˆœí™” ì ìš©")
            
            # ì—°ê²° ë°©ì‹ì— ë”°ë¥¸ ë¡œê¹…
            if self.use_mcp:
                print(f"ğŸ”’ MCP í”„ë¡œí† ì½œì„ í†µí•´ ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
                print(f"   - ëª¨ë¸: {self.model_type}")
                print(f"   - ë³´ì•ˆ: í† í° ê¸°ë°˜ ì¸ì¦")
                print(f"   - í”„ë¡œí† ì½œ: JSON-RPC í‘œì¤€")
            else:
                print(f"ğŸŒ ì¼ë°˜ HTTP APIë¥¼ í†µí•´ ìš”ì²­ ì²˜ë¦¬ ì¤‘...")
                print(f"   - ëª¨ë¸: {self.model_type}")
                print(f"   - ë³´ì•ˆ: ê¸°ë³¸ HTTP")
                print(f"   - í”„ë¡œí† ì½œ: REST API")

            response = self.model.invoke(enhanced_prompt)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # ì„±ëŠ¥ ì •ë³´ ì¶”ê°€
            if self.use_mcp:
                print(f"âœ… MCP ì‘ë‹µ ì™„ë£Œ (ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ)")
                print(f"   - ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬: êµ¬ì¡°í™”ë¨")
                print(f"   - ë³´ì•ˆ ë ˆë²¨: ë†’ìŒ")
            else:
                print(f"âœ… ì¼ë°˜ API ì‘ë‹µ ì™„ë£Œ (ì²˜ë¦¬ì‹œê°„: {processing_time:.2f}ì´ˆ)")
                print(f"   - ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬: ê¸°ë³¸")
                print(f"   - ë³´ì•ˆ ë ˆë²¨: ê¸°ë³¸")
            
            return response.strip()
            
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def simple_chat(self, user_input: str) -> str:
        """ê°„ë‹¨í•œ ì±„íŒ… ì‘ë‹µ ìƒì„±"""
        if not self.is_model_loaded():
            return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            import time
            start_time = time.time()
            
            # ì—°ê²° ë°©ì‹ í‘œì‹œ
            connection_type = "MCP í”„ë¡œí† ì½œ" if self.use_mcp else "ì¼ë°˜ HTTP API"
            print(f"\nğŸ¤– {connection_type}ë¥¼ í†µí•œ ì±„íŒ… ìš”ì²­")
            print(f"   ì‚¬ìš©ì ì…ë ¥: {user_input[:50]}{'...' if len(user_input) > 50 else ''}")
            
            # Qwen 3BëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—†ì´ ì§ˆë¬¸ë§Œ ì „ë‹¬
            enhanced_input = user_input
            
            response = self.model.invoke(enhanced_input)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # ì‘ë‹µ ì •ë³´
            print(f"   ì‘ë‹µ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            print(f"   ì‘ë‹µ ê¸¸ì´: {len(response)} ë¬¸ì")
            print(f"   ì—°ê²° ë°©ì‹: {connection_type}")
            
            return response.strip()
            
        except Exception as e:
            print(f"ì±„íŒ… ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def is_model_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ì—¬ë¶€ í™•ì¸"""
        return self.model is not None

    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        model_info = {
            "model_loaded": self.is_model_loaded(),
            "use_mcp": self.use_mcp,
            "use_ollama_lib": self.use_ollama,
            "model_type": self.model_type,
            "model_path": self.model_path,
            "model_class": self.model.__class__.__name__ if self.is_model_loaded() else None,
            "model_exists": os.path.exists(self.model_path) if self.model_path else False
        }
        return model_info

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ - MCP ì‚¬ìš©ì„ ê¸°ë³¸ìœ¼ë¡œ ì„¤ì • (Qwen 3B ëª¨ë¸ ì‚¬ìš©)
llama_service = LangChainAIService(use_mcp=True, use_ollama=False, model_type="qwen3")

def update_service_settings(use_mcp: bool, model_type: str):
    """ì„œë¹„ìŠ¤ ì„¤ì • ì—…ë°ì´íŠ¸"""
    global llama_service
    llama_service.use_mcp = use_mcp
    llama_service.model_type = model_type
    
    # ì„¤ì •ì— ë”°ë¼ ëª¨ë¸ ì¬ë¡œë“œ
    if use_mcp:
        llama_service.load_mcp_model()
    else:
        llama_service.load_ollama_model()


